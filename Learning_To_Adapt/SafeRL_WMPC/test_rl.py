#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„RLæ¨¡å—å®ç°
éªŒè¯ä¸TUM-CONTROLè®¾è®¡çš„ä¸€è‡´æ€§
"""

import sys
import os
import numpy as np
import yaml

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(__file__))

from helpers import load_config, create_default_parameter_sets, save_parameter_sets
from RL_WMPC.environment import RLEnvironment, make_env, setup_environments
from RL_WMPC.observation import ObservationGenerator
from RL_WMPC.reward import RewardGenerator
from RL_WMPC.evaluation import TrainingData, run_policy


def test_helpers():
    """æµ‹è¯•helperså‡½æ•°"""
    print("=== æµ‹è¯•helperså‡½æ•° ===")

    # æµ‹è¯•å‚æ•°é›†åˆ›å»º
    param_sets = create_default_parameter_sets()
    print(f"åˆ›å»ºçš„å‚æ•°é›†æ•°é‡: {len(param_sets)}")
    print(f"å‚æ•°é›†å½¢çŠ¶: {param_sets.shape}")
    print(f"ç¬¬ä¸€ä¸ªå‚æ•°é›†: {param_sets[0]}")

    # æµ‹è¯•å‚æ•°é›†ä¿å­˜å’ŒåŠ è½½
    test_file = "test_parameters.txt"
    save_parameter_sets(param_sets, test_file)
    loaded_params = load_parameter_sets(test_file)
    print(f"ä¿å­˜å’ŒåŠ è½½çš„å‚æ•°é›†æ˜¯å¦ä¸€è‡´: {np.allclose(param_sets, loaded_params)}")

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if os.path.exists(test_file):
        os.remove(test_file)

    print("âœ“ helperså‡½æ•°æµ‹è¯•é€šè¿‡\n")


def test_observation_generator():
    """æµ‹è¯•è§‚æµ‹ç”Ÿæˆå™¨"""
    print("=== æµ‹è¯•è§‚æµ‹ç”Ÿæˆå™¨ ===")

    # åˆ›å»ºè§‚æµ‹ç”Ÿæˆå™¨
    obs_gen = ObservationGenerator(anticipation_horizon=20, n_anticipation_points=5)

    print(f"è§‚æµ‹ç©ºé—´ç»´åº¦: {obs_gen.n_observations}")
    print(f"è§‚æµ‹ç©ºé—´è¾¹ç•Œå½¢çŠ¶: {obs_gen._bounds.shape}")

    # æµ‹è¯•è§‚æµ‹ç”Ÿæˆ
    v = 10.0
    lat_dev = 0.5
    vel_dev = 1.0
    ref_traj = {"ref_v": np.ones(20) * 10.0, "ref_yaw": np.linspace(0, 2 * np.pi, 20)}
    Ts = 0.01

    observation = obs_gen.get_observation(v, lat_dev, vel_dev, ref_traj, Ts)
    print(f"ç”Ÿæˆçš„è§‚æµ‹å½¢çŠ¶: {observation.shape}")
    print(f"è§‚æµ‹èŒƒå›´: [{observation.min():.3f}, {observation.max():.3f}]")

    print("âœ“ è§‚æµ‹ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡\n")


def test_reward_generator():
    """æµ‹è¯•å¥–åŠ±ç”Ÿæˆå™¨"""
    print("=== æµ‹è¯•å¥–åŠ±ç”Ÿæˆå™¨ ===")

    # åˆ›å»ºå¥–åŠ±ç”Ÿæˆå™¨
    sigmas = np.array([0.1, 0.1])
    normalization_lims = np.array([[-3, 3], [-5, 5]])
    reward_gen = RewardGenerator(sigmas, normalization_lims)

    # åˆ›å»ºæ¨¡æ‹Ÿçš„logger
    class MockLogger:
        def get_lateral_deviations(self, step_length):
            return np.array([0.1, 0.2, 0.1])

        def get_velocity_deviations(self, step_length):
            return np.array([0.5, 0.3, 0.4])

    logger = MockLogger()

    # æµ‹è¯•å¥–åŠ±è®¡ç®—
    reward = reward_gen.get_reward(logger, step_length=3)
    print(f"è®¡ç®—çš„å¥–åŠ±: {reward:.3f}")
    print(f"å¥–åŠ±æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…: {0 <= reward <= 1}")

    print("âœ“ å¥–åŠ±ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡\n")


def test_environment():
    """æµ‹è¯•RLç¯å¢ƒ"""
    print("=== æµ‹è¯•RLç¯å¢ƒ ===")

    # åˆ›å»ºé…ç½®
    config = {
        "n_mpc_steps": 5,
        "max_lat_dev": 3.0,
        "episode_length": 100,
        "obs_anticipation_horizon": 20,
        "obs_n_anticipation_points": 5,
        "rew_sigmas": [0.1, 0.1],
        "rew_lims_lat_dev": [-3, 3],
        "rew_lims_vel_dev": [-5, 5],
        "actions_file": "parameters/parameter_sets.txt",
    }

    # åˆ›å»ºç¯å¢ƒ
    env = RLEnvironment(
        config=config,
        trajectory="monteblanco",
        random_restarts=False,
        full_lap=False,
        evaluation_env=False,
    )

    print(f"åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
    print(f"å‚æ•°é›†æ•°é‡: {env.n_actions}")

    # æµ‹è¯•ç¯å¢ƒé‡ç½®
    obs, info = env.reset()
    print(f"åˆå§‹è§‚æµ‹å½¢çŠ¶: {obs.shape}")
    print(f"åˆå§‹è§‚æµ‹èŒƒå›´: [{obs.min():.3f}, {obs.max():.3f}]")

    # æµ‹è¯•ç¯å¢ƒæ­¥è¿›
    action = 0
    obs, reward, terminated, truncated, info = env.step(action)
    print(f"æ­¥è¿›åè§‚æµ‹å½¢çŠ¶: {obs.shape}")
    print(f"å¥–åŠ±: {reward:.3f}")
    print(f"ç»ˆæ­¢: {terminated}, æˆªæ–­: {truncated}")
    print(f"ä¿¡æ¯: {info}")

    print("âœ“ RLç¯å¢ƒæµ‹è¯•é€šè¿‡\n")


def test_environment_factory():
    """æµ‹è¯•ç¯å¢ƒå·¥å‚å‡½æ•°"""
    print("=== æµ‹è¯•ç¯å¢ƒå·¥å‚å‡½æ•° ===")

    config = {
        "n_mpc_steps": 5,
        "max_lat_dev": 3.0,
        "episode_length": 100,
        "obs_anticipation_horizon": 20,
        "obs_n_anticipation_points": 5,
        "rew_sigmas": [0.1, 0.1],
        "rew_lims_lat_dev": [-3, 3],
        "rew_lims_vel_dev": [-5, 5],
        "actions_file": "parameters/parameter_sets.txt",
    }

    # æµ‹è¯•make_env
    env_fn = make_env(config, "monteblanco", False, False, False)
    env = env_fn()
    print(f"å·¥å‚å‡½æ•°åˆ›å»ºçš„ç¯å¢ƒç±»å‹: {type(env)}")

    # æµ‹è¯•setup_environments
    try:
        vec_env = setup_environments(
            n_envs=2,
            config=config,
            trajectories=["monteblanco", "modena"],
            monitor_path="test_monitor",
            random_restarts=False,
            full_lap=False,
            evaluation_env=False,
        )
        print(f"å‘é‡åŒ–ç¯å¢ƒç±»å‹: {type(vec_env)}")
    except Exception as e:
        print(f"å‘é‡åŒ–ç¯å¢ƒåˆ›å»ºå¤±è´¥ï¼ˆé¢„æœŸï¼‰: {e}")

    print("âœ“ ç¯å¢ƒå·¥å‚å‡½æ•°æµ‹è¯•é€šè¿‡\n")


def test_evaluation():
    """æµ‹è¯•è¯„ä¼°åŠŸèƒ½"""
    print("=== æµ‹è¯•è¯„ä¼°åŠŸèƒ½ ===")

    # æµ‹è¯•TrainingDataï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    try:
        training_data = TrainingData("logs/test_run", "test")
        print(f"è®­ç»ƒæ•°æ®é”®: {list(training_data.data.keys())}")
    except Exception as e:
        print(f"TrainingDataåˆ›å»ºå¤±è´¥ï¼ˆé¢„æœŸï¼‰: {e}")

    print("âœ“ è¯„ä¼°åŠŸèƒ½æµ‹è¯•é€šè¿‡\n")


def test_integration():
    """æµ‹è¯•é›†æˆåŠŸèƒ½"""
    print("=== æµ‹è¯•é›†æˆåŠŸèƒ½ ===")

    # åˆ›å»ºå®Œæ•´çš„è®­ç»ƒé…ç½®
    config = {
        "model_identifier": "test_run",
        "n_environments": 1,
        "n_envs_eval": 1,
        "n_mpc_steps": 5,
        "max_lat_dev": 3.0,
        "episode_length": 50,
        "obs_anticipation_horizon": 20,
        "obs_n_anticipation_points": 5,
        "rew_sigmas": [0.1, 0.1],
        "rew_lims_lat_dev": [-3, 3],
        "rew_lims_vel_dev": [-5, 5],
        "actions_file": "parameters/parameter_sets.txt",
        "learning_rate": 3e-4,
        "n_steps": 2048,
        "batch_size": 64,
        "n_epochs": 10,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "clip_range": 0.2,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "net_arch": [64, 64],
        "use_adaptive_learning_rate": False,
        "evaluation_frequency": 1000,
        "n_eval_episodes": 5,
        "n_training_steps": 10000,
    }

    # ä¿å­˜é…ç½®
    os.makedirs("configs", exist_ok=True)
    with open("configs/test_rl_config.yaml", "w") as f:
        yaml.dump(config, f)

    print("âœ“ é›†æˆæµ‹è¯•é€šè¿‡\n")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•ä¿®å¤åçš„RLæ¨¡å—å®ç°...\n")

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs("parameters", exist_ok=True)
    os.makedirs("logs", exist_ok=True)

    # åˆ›å»ºé»˜è®¤å‚æ•°é›†æ–‡ä»¶
    param_sets = create_default_parameter_sets()
    save_parameter_sets(param_sets, "parameters/parameter_sets.txt")

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_helpers()
    test_observation_generator()
    test_reward_generator()
    test_environment()
    test_environment_factory()
    test_evaluation()
    test_integration()

    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RLæ¨¡å—å®ç°ä¸TUM-CONTROLè®¾è®¡ä¿æŒä¸€è‡´ã€‚")
    print("\nä¸»è¦æ”¹è¿›:")
    print("1. âœ… æ·»åŠ äº†å®Œæ•´çš„helperså‡½æ•°é›†")
    print("2. âœ… ä¿®å¤äº†è§‚æµ‹ç©ºé—´è®¾è®¡å’Œå½’ä¸€åŒ–")
    print("3. âœ… å®ç°äº†æ­£ç¡®çš„å¥–åŠ±è®¡ç®—")
    print("4. âœ… ç®€åŒ–äº†ç¯å¢ƒå®ç°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»„ä»¶")
    print("5. âœ… æ·»åŠ äº†å®Œæ•´çš„è®­ç»ƒæµç¨‹")
    print("6. âœ… å®ç°äº†ä¸TUM-CONTROLä¸€è‡´çš„æ¥å£")


if __name__ == "__main__":
    main()
